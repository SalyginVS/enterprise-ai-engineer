# ADR-001: Выбор модельного стека лаборатории

**Дата:** 21.02.2026  
**Статус:** Принято  
**Автор:** Vladimir (CIO) + Claude (ментор)

---

## Контекст

На старте 9-месячной программы Enterprise AI Agent Engineer требуется определить набор локальных LLM (Large Language Model — большая языковая модель), которые будут использоваться для обучения и прототипирования. Оборудование: NVIDIA RTX 3090 24GB VRAM.

## Проблема

24GB VRAM — это значительный, но конечный ресурс. Нельзя держать все модели одновременно. Нужна чёткая ролевая специализация каждой модели и правило: какую модель использовать для какого типа задач.

## Рассмотренные варианты

**Вариант A: Одна универсальная большая модель**  
Например, только qwen3:30b. Проще управлять, но медленно (6 tok/s) и неэффективно для быстрых итераций при обучении.

**Вариант B: Только маленькие быстрые модели**  
Только qwen3:8b и подобные. Быстро, но не хватает качества для production-level кода.

**Вариант C: Специализированный стек по ролям** ← ПРИНЯТО  
Несколько моделей с чёткими ролями, запускаются по одной (или по задаче).

## Принятое решение

Стек из 6 моделей с чёткой специализацией:

| Модель | Размер | Роль | Когда использовать |
|--------|--------|------|-------------------|
| `qwen3:8b` | ~5.2 GB | Быстрый универсал | Обучение, итерации, чат, лёгкий код |
| `qwen3:14b` | ~9.3 GB | Research / аналитика | Анализ документов, RAG, глубокие объяснения |
| `deepseek-coder-v2:16b` | ~8.9 GB | Основной кодер | Ежедневная генерация кода, рефакторинг |
| `deepseek-r1:14b` | ~9.0 GB | Reasoning | Сложные логические задачи, chain-of-thought |
| `qwen3-coder:30b` | ~18 GB | Тяжёлый кодер | Максимальное качество кода (запуск отдельно) |
| `qwen3-vl:8b` | ~6.1 GB | Мультимодальный | Анализ изображений, скриншотов |

**Правило одновременного запуска:** не более одной модели 14B+ в памяти одновременно при активной нагрузке.

## Последствия

**Плюсы:**
- Оптимальное использование 24GB VRAM
- Каждый тип задачи получает подходящую модель
- Быстрые итерации при обучении (qwen3:8b — высокая скорость)
- Production-quality код по требованию (deepseek-coder-v2)

**Минусы / риски:**
- Нужно помнить какую модель когда использовать
- Переключение между моделями занимает время загрузки
- qwen3-coder:30b потребляет почти всю VRAM, нельзя параллельно

## Критерий пересмотра

ADR пересматривается при: появлении значительно более эффективных моделей в том же размерном классе, изменении аппаратной базы лаборатории, или если текущий стек не обеспечивает нужное качество для задач M3+.

---

*Следующий ADR: ADR-002 — выбор архитектуры LLM Gateway (M1)*

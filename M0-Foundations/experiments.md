# M0: Experiments Log — Журнал экспериментов

> Здесь фиксируются все попытки: удачные и неудачные.  
> Формат: дата · модель · что пробовал · результат · вывод

---

## О модуле M0 и этом журнале

### Контекст и цель модуля

M0 — это недельный boot-camp, точка входа в программу. Исходная позиция: широкий ИТ-опыт, минимальный практический опыт работы с LLM API. Цель на выходе: уверенное понимание того как LLM работает под капотом, и первый рабочий прототип агента-классификатора.

Весь M0 строится вокруг одного бизнес-кейса — классификации инвойсов (счетов-фактур) по категориям расходов. Это не случайный выбор: задача достаточно простая чтобы сосредоточиться на механике LLM, и достаточно реальная чтобы сразу думать про production-ограничения.

### Теоретическая база

**Как работает LLM-вызов.** Модель — это статичный файл весов на диске. Ollama загружает его в VRAM (видеопамять GPU) при первом запросе и держит там пока не вытеснит другая модель или не истечёт таймаут. Каждый запрос — это HTTP POST на порт 11434 с JSON-телом: модель, сообщения, параметры. Ответ возвращается либо потоком (stream: true — токен за токеном), либо целиком (stream: false — ждём полного ответа).

**Токен и скорость генерации.** Токен — это примерно слово или часть слова (в среднем 0.75 слова на токен для английского, чуть меньше для кириллицы). Модель генерирует токены последовательно, один за другим. Скорость измеряется в tok/s (токенов в секунду). На RTX 3090 с qwen3:8b это ~130 tok/s. Важно разделять: время загрузки модели в VRAM (один раз, холодный старт), время обработки промпта (prompt eval), и время генерации ответа (eval). Для reasoning-моделей типа deepseek-r1 добавляется фаза thinking, которую Ollama не включает в eval_duration.

**Три роли сообщений.** Каждый запрос содержит messages — массив объектов с полем role. Роль `system` задаёт контекст и правила поведения модели один раз для всего диалога. Роль `user` — это входящее сообщение от пользователя или агента. Роль `assistant` — предыдущие ответы модели (используется для поддержания контекста диалога). Забытый system prompt — одна из самых частых ошибок: переменная определена, но не передана в messages.

**Temperature (температура).** Параметр от 0.0 до 1.0 управляет случайностью генерации. При temperature=0.0 модель детерминирована — всегда выбирает наиболее вероятный токен. При temperature=1.0 — творческая и разнообразная. Для классификации и structured output используем 0.1: предсказуемость важнее разнообразия.

**Три паттерна промптинга.** В M0 отрабатываются три базовых паттерна которые станут строительными блоками всех последующих агентов. System Prompt + JSON — модель получает роль и формат ответа через system, генерирует структурированный JSON; подходит для стандартных случаев, минимальные токены. Few-Shot (обучение на примерах) — вместо описания правил показываем примеры входа/выхода; модель "понимает" паттерн и применяет его к новым данным; важно покрывать все категории иначе модель изобретает собственные. Chain-of-Thought (цепочка рассуждений) — модель сначала думает пошагово в теге `<thinking>`, потом даёт финальный ответ; точнее на сложных пограничных случаях, но дороже по токенам и времени.

**Проблема калибровки уверенности.** LLM не умеют честно оценивать собственную уверенность. Модель возвращает confidence=0.95 на очевидных кейсах и на пограничных одинаково. Это структурная особенность, не баг конкретной модели. Решение — валидация не через числовой confidence, а через структурные признаки: соответствие enum, наличие признаков неопределённости в тексте reasoning.

### Что делается в M0: карта экспериментов

| EXP | Что проверяем | Ключевой вывод |
|-----|---------------|----------------|
| EXP-000 | Ollama API через curl | Базовая связность стенда |
| EXP-001 | Сетевой доступ с ноутбука | OLLAMA_HOST=0.0.0.0 обязателен |
| EXP-002 | Python HTTP vs библиотека | Холодный старт vs прогретая VRAM |
| EXP-003 | Сравнение моделей, токены | qwen3:8b быстрый, deepseek-r1 думает |
| EXP-004 | System Prompt + JSON | Забытый system = сломанный формат |
| EXP-005 | Few-Shot примеры | Неполное покрытие = галлюцинация категорий |
| EXP-006 | Chain-of-Thought | CoT точнее но дороже, CAPEX требует бизнес-правил |
| EXP-007 | Smart Pipeline | Промпт влияет на результат не меньше данных |

### Связь с последующими модулями

Все проблемы обнаруженные в M0 получат системное решение в M1: Pydantic-валидация закроет проблему невалидных категорий (EXP-005), структурный escalation заменит ненадёжный confidence (EXP-007), retry-логика обработает нестабильный JSON (EXP-004). Smart Pipeline из EXP-007 станет архитектурным скелетом Invoice Classifier Agent в M1.

---

## EXP-000: Первый вызов через curl (до Python)

**Дата:** 25.02.2026  
**Модель:** qwen3:8b  
**Цель:** убедиться что Ollama API отвечает

```bash
# Команда
curl -s http://127.0.0.1:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen3:8b", "prompt": "Скажи привет", "stream": false}'
```

**Результат:** Положительный.  
**Вывод:** Ответ получен

---
## EXP-001: Первый Python вызов

**Дата:** 25.02.2026  
**Модель:** qwen3:8b  
**Файл:** src/01_raw_http.py
**Цель:** Python → Ollama API → получить ответ

**EXP-001** — результат: `Ollama is running` в браузере после настройки `OLLAMA_HOST=0.0.0.0`. 
Вывод: по умолчанию Ollama слушает только localhost, для сетевого доступа нужен override.conf. 

---
## EXP-002: Второй Python вызов с библиотекой

**Дата:** 25.02.2026    
**Модель:** qwen3:8b  
**Файл:** src/01_raw_http.py и  src/ 02_ollama_library.py
**Цель:** Python → Ollama API → получить ответ

**EXP-002** — результат: ответ получен. Время: 6.6 сек (первый вызов, холодный старт) и 2.4 сек (через библиотеку, модель уже в VRAM). 
Вывод: первый вызов включает загрузку модели, последующие — только генерацию.

---
## EXP-003: Сравнение моделей на одном промпте

Дата: 25.02.2026
Файл: src/03_token_stats.py

| Модель          | tok/s | Токенов | Общее время |
|-----------------|-------|---------|-------------|
| qwen3:8b        | 130.9 | 273     | 3.9s        |
| deepseek-r1:14b | 82.6  | 268     | 20.2s       |

Ключевой инсайт: у deepseek-r1:14b генерация заняла 3.24с,
но общее время 20.2с — 17 секунд ушло на блок <think>
(внутреннее рассуждение до финального ответа).
Ollama не включает thinking-фазу в eval_duration.

Вывод: qwen3:8b для быстрых итераций, deepseek-r1:14b
только для задач где нужно глубокое рассуждение.

---

## EXP-004: Сравнение моделей на одном промпте

Дата: 25.02.2026
Файл: src/04_system_prompt.py

Результат: все 3 инвойса классифицированы верно (office_supplies, services, utilities) Многоязычность: украинский инвойс обработан корректно 

Грабли: забыл передать SYSTEM_PROMPT в messages — модель отвечала свободным текстом вместо JSON. JSON parser вернул пустую строку. 

Урок: system prompt определить мало — нужно явно передать в messages. Наблюдение: confidence всегда 0.95 — LLM плохо калибруют уверенность. 

Решение в M1: валидация через Pydantic + пороговые значения.

---

## EXP-005: Few-Shot Examples — пограничные случаи

Дата: 25.02.2026
Файл: src/05_few_shot.py

Microsoft 365 → services ✅ (пограничный кейс решён верно)
Lenovo ThinkPad → electronics ⚠️  (ожидали equipment)
Курьерська доставка → services ✅ (украинский работает)

ГРАБЛИ: модель изобрела категорию "electronics" которой нет в списке.
Причина: в few-shot примерах не было примера для категории "equipment".
Модель заполнила пробел своей логикой — нарушила контракт формата.

Урок: few-shot должен покрывать ВСЕ категории.

Решение в M1: Pydantic enum — жёсткая валидация на уровне кода.

---

## EXP-006: Chain-of-Thought — сложные пограничные случаи

Дата: 25.02.2026
Файл: src/06_chain_of_thought.py

Кейс 1: Dell PowerEdge + монтаж + поддержка → services
  Ожидалось: equipment (CAPEX логика)
  Модель сосредоточилась на глаголах (налаштування, підтримка)
  и перевесила в сторону услуг. Логически связно, но спорно.
  Вывод: пограничные CAPEX-кейсы нужно решать бизнес-правилами,
  а не полагаться только на LLM.

Кейс 2: Herman Miller chairs x20 → equipment ✅
  Верно разграничил office_supplies (канцелярия) vs equipment (мебель).
  Верно проигнорировал доставку как не меняющую категорию.

Сравнение паттернов по токенам и скорости:
  System Prompt + JSON : ~30 токенов,  ~3-4s  — стандартные кейсы
  Few-Shot             : ~100 токенов, ~5-8s  — нужен точный формат
  Chain-of-Thought     : ~200+ токенов, ~10s  — сложные/пограничные

Вывод: в production комбинировать — быстрая классификация сначала,
при низком confidence escalation на CoT.

---

## EXP-007: Smart Pipeline — автовыбор стратегии

Дата: 25.02.2026
Файл: src/07_smart_pipeline.py

Результат: 4/4 валидных, 0 escalations, 19.2s, 2659 токенов

Наблюдение 1: Dell PowerEdge → equipment (fast), но ранее в EXP-006
  тот же инвойс через CoT дал services. Разные промпты = разные ответы.
  Вывод: промпт влияет на результат не меньше чем данные.
  Решение в M6: eval pipeline для сравнения версий промптов.

Наблюдение 2: escalation не сработал ни разу — модель всегда
  возвращает confidence 0.90-0.95, никогда ниже порога 0.85.
  
  Причина: LLM плохо калибруют уверенность (проблема из EXP-004).
  
  Решение в M1: escalation через структурные признаки
  (невалидный enum, слова "unclear/ambiguous" в reasoning).

---

## EXP-008: Первый Tool Call через Ollama

**Дата:** 2026-02-26
**Модель:** qwen3:14b
**Скрипт:** src/08_first_tool.py

**Цель:** Освоить механизм tool calling — базовый паттерн любого агента.

**Ключевые наблюдения:**
- Полный цикл tool calling работает: вопрос → tool request → выполнение → результат → финальный ответ
- Когда модель решает вызвать инструмент: content='' и tool_calls=[...] — это нормальное поведение протокола
- Каждый tool call получает уникальный id ('call_ofrfxano' и т.д.) — важно для будущего параллельного вызова нескольких инструментов
- Один tool use = два HTTP-запроса к модели: первый для принятия решения, второй для формулировки финального ответа
- Edge case отработан корректно: несуществующий INV-2024-999 вернул {"status": "Не найден"}, модель корректно транслировала это пользователю без галлюцинаций
- Выбор qwen3:14b вместо 8b оправдан: модель ни разу не попыталась ответить из собственных знаний, всегда обращалась к инструменту

**Архитектурный инсайт:**
Модель не выполняет инструменты — она только принимает решение о вызове и формулирует запрос.
Весь execution остаётся на стороне Python-кода. Это критически важно для безопасности:
именно здесь в продакшне будут access control, валидация аргументов и аудит-лог.

**Результат:** PASSED — все 3 тест-кейса, включая not-found сценарий

---


# M0: Experiments Log — Журнал экспериментов

> Здесь фиксируются все попытки: удачные и неудачные.  
> Формат: дата · модель · что пробовал · результат · вывод

---

## EXP-000: Первый вызов через curl (до Python)

**Дата:** 25.02.2026  
**Модель:** qwen3:8b  
**Цель:** убедиться что Ollama API отвечает

```bash
# Команда
curl -s http://127.0.0.1:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen3:8b", "prompt": "Скажи привет", "stream": false}'
```

**Результат:** Положительный.  
**Вывод:** Ответ получен

---
## EXP-001: Первый Python вызов

**Дата:** 25.02.2026  
**Модель:** qwen3:8b  
**Файл:** src/01_raw_http.py
**Цель:** Python → Ollama API → получить ответ

**EXP-001** — результат: `Ollama is running` в браузере после настройки `OLLAMA_HOST=0.0.0.0`. 
Вывод: по умолчанию Ollama слушает только localhost, для сетевого доступа нужен override.conf. 

---
## EXP-002: Второй Python вызов с библиотекой

**Дата:** 25.02.2026    
**Модель:** qwen3:8b  
**Файл:** src/01_raw_http.py и  src/ 02_ollama_library.py
**Цель:** Python → Ollama API → получить ответ

**EXP-002** — результат: ответ получен. Время: 6.6 сек (первый вызов, холодный старт) и 2.4 сек (через библиотеку, модель уже в VRAM). 
Вывод: первый вызов включает загрузку модели, последующие — только генерацию.

---
## EXP-003: Сравнение моделей на одном промпте

Дата: 25.02.2026
Файл: src/03_token_stats.py

| Модель          | tok/s | Токенов | Общее время |
|-----------------|-------|---------|-------------|
| qwen3:8b        | 130.9 | 273     | 3.9s        |
| deepseek-r1:14b | 82.6  | 268     | 20.2s       |

Ключевой инсайт: у deepseek-r1:14b генерация заняла 3.24с,
но общее время 20.2с — 17 секунд ушло на блок <think>
(внутреннее рассуждение до финального ответа).
Ollama не включает thinking-фазу в eval_duration.

Вывод: qwen3:8b для быстрых итераций, deepseek-r1:14b
только для задач где нужно глубокое рассуждение.

---

## EXP-004: Сравнение моделей на одном промпте

Дата: 25.02.2026
Файл: src/04_system_prompt.py

Результат: все 3 инвойса классифицированы верно (office_supplies, services, utilities) Многоязычность: украинский инвойс обработан корректно 

Грабли: забыл передать SYSTEM_PROMPT в messages — модель отвечала свободным текстом вместо JSON. JSON parser вернул пустую строку. 

Урок: system prompt определить мало — нужно явно передать в messages. Наблюдение: confidence всегда 0.95 — LLM плохо калибруют уверенность. 

Решение в M1: валидация через Pydantic + пороговые значения.

---

## EXP-005: Few-Shot Examples — пограничные случаи

Дата: 25.02.2026
Файл: src/05_few_shot.py

Microsoft 365 → services ✅ (пограничный кейс решён верно)
Lenovo ThinkPad → electronics ⚠️  (ожидали equipment)
Курьерська доставка → services ✅ (украинский работает)

ГРАБЛИ: модель изобрела категорию "electronics" которой нет в списке.
Причина: в few-shot примерах не было примера для категории "equipment".
Модель заполнила пробел своей логикой — нарушила контракт формата.

Урок: few-shot должен покрывать ВСЕ категории.

Решение в M1: Pydantic enum — жёсткая валидация на уровне кода.

---

## EXP-006: Chain-of-Thought — сложные пограничные случаи

Дата: 25.02.2026
Файл: src/06_chain_of_thought.py

Кейс 1: Dell PowerEdge + монтаж + поддержка → services
  Ожидалось: equipment (CAPEX логика)
  Модель сосредоточилась на глаголах (налаштування, підтримка)
  и перевесила в сторону услуг. Логически связно, но спорно.
  Вывод: пограничные CAPEX-кейсы нужно решать бизнес-правилами,
  а не полагаться только на LLM.

Кейс 2: Herman Miller chairs x20 → equipment ✅
  Верно разграничил office_supplies (канцелярия) vs equipment (мебель).
  Верно проигнорировал доставку как не меняющую категорию.

Сравнение паттернов по токенам и скорости:
  System Prompt + JSON : ~30 токенов,  ~3-4s  — стандартные кейсы
  Few-Shot             : ~100 токенов, ~5-8s  — нужен точный формат
  Chain-of-Thought     : ~200+ токенов, ~10s  — сложные/пограничные

Вывод: в production комбинировать — быстрая классификация сначала,
при низком confidence escalation на CoT.

---

## EXP-007: Smart Pipeline — автовыбор стратегии

Дата: 25.02.2026
Файл: src/07_smart_pipeline.py

Результат: 4/4 валидных, 0 escalations, 19.2s, 2659 токенов

Наблюдение 1: Dell PowerEdge → equipment (fast), но ранее в EXP-006
  тот же инвойс через CoT дал services. Разные промпты = разные ответы.
  Вывод: промпт влияет на результат не меньше чем данные.
  Решение в M6: eval pipeline для сравнения версий промптов.

Наблюдение 2: escalation не сработал ни разу — модель всегда
  возвращает confidence 0.90-0.95, никогда ниже порога 0.85.
  
  Причина: LLM плохо калибруют уверенность (проблема из EXP-004).
  
  Решение в M1: escalation через структурные признаки
  (невалидный enum, слова "unclear/ambiguous" в reasoning).
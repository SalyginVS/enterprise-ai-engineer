# 03_token_stats.py
# –¶–µ–ª—å: –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–æ–∫–µ–Ω–æ–≤ –∏ —Å–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
# –≠—Ç–æ –æ—Å–Ω–æ–≤–∞ –¥–ª—è –±—É–¥—É—â–µ–≥–æ FinOps –∏ –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥–∞ –º–æ–¥–µ–ª–µ–π

from ollama import Client
import time

client = Client(host="http://192.168.0.128:11434")

def call_with_stats(model: str, prompt: str):
    """–í—ã–∑–æ–≤ —Å –ø–æ–ª–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π —Ç–æ–∫–µ–Ω–æ–≤"""
    
    print(f"–ú–æ–¥–µ–ª—å: {model}")
    print(f"–ü—Ä–æ–º–ø—Ç: {prompt[:60]}...")
    print("-" * 50)
    
    start_time = time.time()
    
    response = client.chat(
        model=model,
        messages=[{"role": "user", "content": prompt}]
    )
    
    elapsed = time.time() - start_time
    
    # –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
    print(f"–û—Ç–≤–µ—Ç:\n{response.message.content}")
    print("-" * 50)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –æ—Ç–≤–µ—Ç–∞
    # eval_count = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
    # eval_duration = –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –Ω–∞–Ω–æ—Å–µ–∫—É–Ω–¥–∞—Ö
    # prompt_eval_count = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø—Ä–æ–º–ø—Ç–µ
    
    eval_count = response.eval_count or 0
    eval_duration_ns = response.eval_duration or 1  # –Ω–∞–Ω–æ—Å–µ–∫—É–Ω–¥—ã
    prompt_tokens = response.prompt_eval_count or 0
    
    # –ü–µ—Ä–µ–≤–æ–¥–∏–º –Ω–∞–Ω–æ—Å–µ–∫—É–Ω–¥—ã –≤ —Å–µ–∫—É–Ω–¥—ã (1 —Å–µ–∫—É–Ω–¥–∞ = 1_000_000_000 –Ω—Å)
    eval_duration_sec = eval_duration_ns / 1_000_000_000
    
    # –°–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    tokens_per_second = eval_count / eval_duration_sec if eval_duration_sec > 0 else 0
    
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤:")
    print(f"  –¢–æ–∫–µ–Ω–æ–≤ –≤ –ø—Ä–æ–º–ø—Ç–µ:    {prompt_tokens}")
    print(f"  –¢–æ–∫–µ–Ω–æ–≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {eval_count}")
    print(f"  –ò—Ç–æ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤:        {prompt_tokens + eval_count}")
    print(f"  –°–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:   {tokens_per_second:.1f} tok/s")
    print(f"  –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:      {eval_duration_sec:.2f} —Å–µ–∫")
    print(f"  –û–±—â–µ–µ –≤—Ä–µ–º—è:          {elapsed:.2f} —Å–µ–∫")
    
    return {
        "model": model,
        "prompt_tokens": prompt_tokens,
        "completion_tokens": eval_count,
        "tokens_per_second": tokens_per_second,
        "elapsed": elapsed
    }


def compare_models():
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ–¥–Ω–æ–º –ø—Ä–æ–º–ø—Ç–µ"""
    
    prompt = "–û–±—ä—è—Å–Ω–∏ —á—Ç–æ —Ç–∞–∫–æ–µ AI Agent –≤ 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö. –û—Ç–≤–µ—á–∞–π –ø–æ-—Ä—É—Å—Å–∫–∏."
    
    models = ["qwen3:8b", "deepseek-r1:14b"]
    results = []
    
    for model in models:
        print(f"\n{'='*50}")
        stats = call_with_stats(model, prompt)
        results.append(stats)
        print()
    
    # –ò—Ç–æ–≥–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    print(f"\n{'='*50}")
    print("üìà –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π:")
    print(f"{'–ú–æ–¥–µ–ª—å':<25} {'tok/s':>8} {'–¢–æ–∫–µ–Ω–æ–≤':>10} {'–í—Ä–µ–º—è':>8}")
    print("-" * 55)
    for r in results:
        print(
            f"{r['model']:<25} "
            f"{r['tokens_per_second']:>7.1f} "
            f"{r['completion_tokens']:>10} "
            f"{r['elapsed']:>7.1f}s"
        )


if __name__ == "__main__":
    compare_models()